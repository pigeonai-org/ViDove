# import argparse
# import pandas as pd
from scores.multi_scores import cal_all_scores
from scores.score import SubERscore



# #     3. eval 的主程序把结果和数据集每行加入src_list / mt_list / ref_list 变量，传入multiscore去做评判，return的结果在eval通过csv储存
    
    
# class Evaluator:
#     def __init__(self, pred_path, gt_path, eval_path, res_path):
#         self.pred_path = pred_path # the pred result generated by our agent
#         self.gt_path = gt_path # the ground truth result (from dataset)
#         self.eval_path = eval_path # the evaluation result (bleu, comet, llm)
#         self.res_path = res_path # the result of average scores (bleu, comet, llm)


#     def eval(self):
#         # Align two SRT files
        
#         # TODO: dataset都是短视频，所以所有pred_s的内容都只需要align到一行而不需要现在的align函数，而gt_s的source_text和translation是分别分布在两个文件里面的

#         # Get sentence scores
#         scorer = multi_scores()
#         result_data = []
#         for (pred_s, gt_s) in aligned_srt:
#             print("pred_s.source_text: ", pred_s.source_text)
#             print("pred_s.translation: ", pred_s.translation)
#             print("gt_s.source_text: ", gt_s.source_text)
#             print("gt_s.translation: ", gt_s.translation)

#             # Check if the gt_s.translation is not empty
#             if gt_s.translation != "":
#                 # gt_s.translation = " "
#                 scores_dict = scorer.get_scores(pred_s.source_text, pred_s.translation, gt_s.translation)
#             else:
#                 scores_dict = scorer.get_scores(pred_s.source_text, pred_s.translation, gt_s.source_text)

#             print("scores_dict: ", scores_dict)

#             scores_dict['Source'] = pred_s.source_text
#             scores_dict['Prediction'] = pred_s.translation
#             scores_dict['Ground Truth'] = gt_s.source_text
#             result_data.append(scores_dict)

#         eval_df = pd.DataFrame(result_data)
#         eval_df.to_csv(self.eval_path, index=False, columns=['Source', 'Prediction', 'Ground Truth', 'bleu_score', 'comet_score', 'llm_score', 'llm_explanation'])

#         # Get average scores
#         avg_llm = eval_df['llm_score'].mean()
#         avg_bleu = eval_df['bleu_score'].mean()
#         avg_comet = eval_df['comet_score'].mean()

#         res_data = {
#             'Metric': ['Avg LLM', 'Avg BLEU', 'Avg COMET'],
#             'Score': [avg_llm, avg_bleu, avg_comet]
#         }
#         res_df = pd.DataFrame(res_data)
#         res_df.to_csv(self.res_path, index=False)

# if __name__ == "__main__":
#     parser = argparse.ArgumentParser(description='Evaluate SRT files.')
#     parser.add_argument('-bi_path', default='evaluation/test5_tiny/test5_bi.srt', help='Path to predicted SRT file')
#     parser.add_argument('-zh_path', default='evaluation/test5_tiny/test5_gt.srt', help='Path to ground truth SRT file')
#     parser.add_argument('-eval_output', default='evaluation/test5_tiny/eval.csv', help='Path to eval CSV file')
#     parser.add_argument('-res_output', default='evaluation/test5_tiny/res.csv', help='Path to result CSV file')
#     args = parser.parse_args()

#     evaluator = Evaluator(args.bi_path, args.zh_path, args.eval_output, args.res_output)
#     evaluator.eval()


# # python evaluation.py -bi_path /home/jiaenliu/project-t/results/test1/test1_bi.srt -zh_path test5_tiny/test1_gt.srt -eval_output /home/jiaenliu/project-t/evaluation/results/test1_large/eval.csv -res_output /home/jiaenliu/project-t/evaluation/results/test1_large/res.csv

def load_data(src_file, mt_file, ref_file):
    """
    Load data from source, machine translation, and reference files.
    Explicitly use UTF-8 encoding to prevent decoding errors.
    
    Args:
        src_file: Path to source text file
        mt_file: Path to machine translation file
        ref_file: Path to reference translation file
        
    Returns:
        Tuple of lists containing the lines from each file
    """
    with open(src_file, 'r', encoding='utf-8') as f:
        src_list = f.readlines()
    
    with open(mt_file, 'r', encoding='utf-8') as f:
        mt_list = f.readlines()
    
    with open(ref_file, 'r', encoding='utf-8') as f:
        ref_list = f.readlines()
    
    # Strip newlines and whitespace
    src_list = [s.strip() for s in src_list]
    mt_list = [m.strip() for m in mt_list]
    ref_list = [r.strip() for r in ref_list]
    
    # Ensure all lists have the same length
    min_len = min(len(src_list), len(mt_list), len(ref_list))
    if len(src_list) != min_len or len(mt_list) != min_len or len(ref_list) != min_len:
        print(f"Warning: Input files have different lengths. Truncating to {min_len} lines.")
        src_list = src_list[:min_len]
        mt_list = mt_list[:min_len]
        ref_list = ref_list[:min_len]
    
    return src_list, mt_list, ref_list

if __name__ == "__main__":
    src_list, mt_list, ref_list = load_data("./evaluation/test_data/text_data_test.en", "./evaluation/test_data/gemini_eval_result.zh", "./evaluation/test_data/text_data_test.zh")
    print("-----------data loaded-----------")
    cal_all_scores(src_list, mt_list, ref_list, csv_path="./evaluation/test_data/gemini_result.csv")
    # cal_all_scores(src_list, mt_list, ref_list, csv_path="./evaluation/test_data/gemini_result.csv", scomet_enabled=True, dcomet_enabled=True, bleu_enabled=True, llm_enabled=False)